{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Reproducing the Experiment in \"Prediction of Pedestrian Speed with Artificial Neural Networks\" by Tordeux et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Enable these if automatic reloading of modules is wanted\n",
    "\n",
    "# Load extension for automatic reload of modules\n",
    "%load_ext autoreload\n",
    "# Enable autoreload for all modules\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import tensorboard\n",
    "\n",
    "import preprocessing\n",
    "import plotting\n",
    "import pedestrian_dataset\n",
    "import pedestrian_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tensorboard\n",
    "\n",
    "Extension for visualizing the training results.\n",
    "Should only be loaded once, otherwise there is probably an error message.\n",
    "\n",
    "To start, run `tensorboard --logdir=dir --port 6006` in a terminal or run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir .name/lightning_logs --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logging\n",
    "\n",
    "I used Logging to print messages.\n",
    "If more messages are welcome, use the logging level `logging.INFO` or even `logging.DEBUG`.\n",
    "If not, use `logging.WARNING`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Set Logging Level\n",
    "logger_format = '%(levelname)s - %(funcName)s \\t%(message)s'\n",
    "logger_level = logging.WARNING\n",
    "logging.basicConfig(level=logger_level, format=logger_format)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'distances': array([ 296.80893951,   36.1585    ,  -91.69      ,   76.8931    ,\n",
      "         62.0273    ,   -2.372     ,  201.1667    ,  -21.5438    ,\n",
      "       -241.436     ,   29.8531    , -262.229     ,   32.8202    ,\n",
      "        295.624     ,  -18.6053    ,  316.108     ,  -26.2509    ,\n",
      "        437.719     ,   43.2941    ,  445.954     ,   -0.6295    ,\n",
      "        559.384     ]), 'speed': 6.112364455756874}\n"
     ]
    }
   ],
   "source": [
    "# Set a torch seed\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing and Loading the Dataset\n",
    "\n",
    "The different files of data in the format\n",
    "(`PedID FrameID X Y Z`)\n",
    "are loaded and converted into the following dictionary format:\n",
    "\n",
    "`distances` | `speeds`\n",
    "-|-\n",
    "Input of our neural network. Array of size $2k+1$ containing the median speed of the $k$ nearest neighbors as the first element and the relative $x$- and $y$-positions of the $k$ nearest neighbors in the following pattern afterwards: $x_1$, $y_1$, $x_2$, $y_2$, ... | Truth value for our neural network. The speed that the pedestrian had in that frame.\n",
    "\n",
    "To load a list of files, the method `pedestrian_dataset.create_dataset()` is used.\n",
    "As its first parameter it either takes a list of data files that it should load\n",
    "or a `pedestrian_dataset.PedestrianDataType` value,\n",
    "which can be either `BOTTLENECK`, which loads all bottleneck files,\n",
    "`CORRIDOR`, which loads all corridor files,\n",
    "or `ALL`, which loads all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'distances': array([ 83.28914988, -32.428     ,  24.169     ,  37.9846    ,\n",
      "        38.714     ,  -2.5528    , -57.585     ,   4.0326    ,\n",
      "        69.008     ,  81.5544    ,   6.506     , -71.6767    ,\n",
      "       -54.426     ,  49.6994    , -84.303     , -57.71222   ,\n",
      "        80.09      ,  86.0264    , -55.428     ,  80.5664    ,\n",
      "        75.733     ]), 'speed': 2.5783409394414765}\n"
     ]
    }
   ],
   "source": [
    "# Creating datasets with only the smallest corridor scenario with 30 participants\n",
    "c_015_path = \"./Data/Corridor_Data/ug-180-030.txt\"\n",
    "# Note: even when only loading one dataset, it has to be given in a list\n",
    "c_015_train_val_datasets, c_015_test_dataset = pedestrian_dataset.create_dataset([c_015_path])\n",
    "\n",
    "# Print the first item from the first train/val dataset part\n",
    "print(c_015_train_val_datasets[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'distances': tensor([[ 83.2891, -32.4280,  24.1690,  37.9846,  38.7140,  -2.5528, -57.5850,\n",
      "           4.0326,  69.0080,  81.5544,   6.5060, -71.6767, -54.4260,  49.6994,\n",
      "         -84.3030, -57.7122,  80.0900,  86.0264, -55.4280,  80.5664,  75.7330]],\n",
      "       dtype=torch.float64), 'speed': tensor([2.5783], dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "# Create a PyTorch dataloader with the dataset\n",
    "\n",
    "# TODO: I don't know how to do cross validation, so we combine the first 4 train/val datasets\n",
    "#   to build the train dataset and use the last train/val dataset as the val dataset\n",
    "#   Maybe we just have to do this everytime (and switch it up)? Could be, but not sure\n",
    "c_015_temp_train_dataset = torch.utils.data.ConcatDataset(c_015_train_val_datasets[:4])\n",
    "c_015_temp_val_dataset = c_015_train_val_datasets[4]\n",
    "\n",
    "c_015_train_loader = DataLoader(c_015_temp_train_dataset, batch_size=batch_size, drop_last=True)\n",
    "c_015_val_loader = DataLoader(c_015_temp_train_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "c_015_test_loader = DataLoader(c_015_test_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "# Print the first value given by the train loader\n",
    "for item in c_015_train_loader:\n",
    "    print(item)\n",
    "    break  # break after printing the first item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating datasets with all scenarios loaded\n",
    "all_train_val_datasets, all_test_dataset = pedestrian_dataset.create_dataset(\n",
    "    pedestrian_dataset.PedestrianDataType.ALL\n",
    ")\n",
    "\n",
    "# Print the first item from the first train/val dataset part\n",
    "print(all_train_val_datasets[0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Create a PyTorch dataloader with the dataset\n",
    "\n",
    "# TODO: I don't know how to do cross validation, so we combine the first 4 train/val datasets\n",
    "#   to build the train dataset and use the last train/val dataset as the val dataset\n",
    "#   Maybe we just have to do this everytime (and switch it up)? Could be, but not sure\n",
    "all_temp_train_dataset = torch.utils.data.ConcatDataset(all_train_val_datasets[:4])\n",
    "all_temp_val_dataset = all_train_val_datasets[4]\n",
    "\n",
    "all_train_loader = DataLoader(all_temp_train_dataset, batch_size=batch_size, drop_last=True)\n",
    "all_val_loader = DataLoader(all_temp_train_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "all_test_loader = DataLoader(all_test_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "# # Currently Disabled because for batch_size=16 this get's large\n",
    "# # Print the first value given by the train loader\n",
    "# for item in all_train_loader:\n",
    "#     print(item)\n",
    "#     break  # break after printing the first item"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementing and Training the Model\n",
    "\n",
    "Now we need to define our model.\n",
    "\n",
    "@Parim:\n",
    "I got quite a lot of work done on the model, but there are still TODOs.\n",
    "I probably won't be able to work more on the practicum this week, but I think everything should be documented well enough to be extended by you.\n",
    "(I'm using PyTorch Lightning now, which you'll probably remember from I2DL, and the Neural Network is in `pedestrian_net.py`.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_name = \"./.name/checkpoints/2023-07-05--dataAll-ep100-it001.ckpt\"\n",
    "\n",
    "max_epochs = 100\n",
    "k = 10\n",
    "hidden_size = 3\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"CPU will be used.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Choose our dataloaders\n",
    "train_loader = all_train_loader\n",
    "val_loader   = all_val_loader\n",
    "test_loader  = all_test_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define an early stopping callback\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode='min', patience=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define our model\n",
    "model = pedestrian_net.PedestrianNet(k=k,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     learning_rate=learning_rate,\n",
    "                                     optimizer=optimizer\n",
    "                                     )\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    devices='auto',\n",
    "    accelerator='gpu',\n",
    "    callbacks=[early_stop_callback],\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(checkpoint_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the Model\n",
    "\n",
    "**After** training and tuning the model, we can test the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trainer.test(model, dataloaders=test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TODO\n",
    "\n",
    "A list of some things that are still to do.\n",
    "Not a complete list.\n",
    "\n",
    "- [x] Implement Preprocessing\n",
    "- [x] Implement Data Preparation\n",
    "- [x] Implement Model basic structure\n",
    "- [x] Set up basic training for model\n",
    "- [ ] Implement cross validation\n",
    "- [ ] Implement hyper parameter tuning\n",
    "- [ ] ...\n",
    "- [ ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}