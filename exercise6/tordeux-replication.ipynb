{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reproducing the Experiment in \"Prediction of Pedestrian Speed with Artificial Neural Networks\" by Tordeux et al."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Enable these if automatic reloading of modules is wanted\n",
    "\n",
    "# Load extension for automatic reload of modules\n",
    "%load_ext autoreload\n",
    "# Enable autoreload for all modules\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "\n",
    "import preprocessing\n",
    "import plotting\n",
    "import pedestrian_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logging\n",
    "\n",
    "I used Logging to print messages.\n",
    "If more messages are welcome, use the logging level `logging.INFO` or even `logging.DEBUG`.\n",
    "If not, use `logging.WARNING`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Set Logging Level\n",
    "logger_format = '%(levelname)s - %(funcName)s \\t%(message)s'\n",
    "logger_level = logging.WARNING\n",
    "logging.basicConfig(level=logger_level, format=logger_format)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing and Loading the Dataset\n",
    "\n",
    "The different files of data in the format\n",
    "(`PedID FrameID X Y Z`)\n",
    "are loaded and converted into the following dictionary format:\n",
    "\n",
    "`distances` | `speeds`\n",
    "-|-\n",
    "Input of our neural network. Array of size $2k+1$ containing the median speed of the $k$ nearest neighbors as the first element and the relative $x$- and $y$-positions of the $k$ nearest neighbors in the following pattern afterwards: $x_1$, $y_1$, $x_2$, $y_2$, ... | Truth value for our neural network. The speed that the pedestrian had in that frame.\n",
    "\n",
    "To load a list of files, the method `pedestrian_dataset.create_dataset()` is used.\n",
    "As its first parameter it either takes a list of data files that it should load\n",
    "or a `pedestrian_dataset.PedestrianDataType` value,\n",
    "which can be either `BOTTLENECK`, which loads all bottleneck files,\n",
    "`CORRIDOR`, which loads all corridor files,\n",
    "or `ALL`, which loads all files."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: Docstrings for the rest of the functions will be added before the final deadline (and imports cleaned up etc.). The most important docstring for `create_dataset()` is already created."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'distances': array([ 296.80893951,   36.1585    ,  -91.69      ,   76.8931    ,\n",
      "         62.0273    ,   -2.372     ,  201.1667    ,  -21.5438    ,\n",
      "       -241.436     ,   29.8531    , -262.229     ,   32.8202    ,\n",
      "        295.624     ,  -18.6053    ,  316.108     ,  -26.2509    ,\n",
      "        437.719     ,   43.2941    ,  445.954     ,   -0.6295    ,\n",
      "        559.384     ]), 'speed': 6.112364455756874}\n"
     ]
    }
   ],
   "source": [
    "# Creating datasets with only the smallest corridor scenario with 30 participants\n",
    "c_015_path = \"./Data/Corridor_Data/ug-180-030.txt\"\n",
    "# Note: even when only loading one dataset, it has to be given in a list\n",
    "c_015_train_val_datasets, c_015_test_dataset = pedestrian_dataset.create_dataset([c_015_path])\n",
    "\n",
    "# Print the first item from the first train/val dataset part\n",
    "print(c_015_train_val_datasets[0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'distances': tensor([[ 296.8089,   36.1585,  -91.6900,   76.8931,   62.0273,   -2.3720,\n",
      "          201.1667,  -21.5438, -241.4360,   29.8531, -262.2290,   32.8202,\n",
      "          295.6240,  -18.6053,  316.1080,  -26.2509,  437.7190,   43.2941,\n",
      "          445.9540,   -0.6295,  559.3840]], dtype=torch.float64), 'speed': tensor([6.1124], dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch dataloader with the dataset\n",
    "\n",
    "# TODO: I don't know how to do cross validation, so we combine the first 4 train/val datasets\n",
    "#   to build the train dataset and use the last train/val dataset as the val dataset\n",
    "#   Maybe we just have to do this everytime (and switch it up)? Could be, but not sure\n",
    "c_015_temp_train_dataset = torch.utils.data.ConcatDataset(c_015_train_val_datasets[:4])\n",
    "c_015_temp_val_dataset = c_015_train_val_datasets[4]\n",
    "\n",
    "c_015_train_loader = DataLoader(c_015_temp_train_dataset)\n",
    "c_015_val_loader = DataLoader(c_015_temp_train_dataset)\n",
    "\n",
    "# Print the first value given by the train loader\n",
    "for item in c_015_train_loader:\n",
    "    print(item)\n",
    "    break  # break after printing the first item"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'distances': array([ 83.28914988, -32.428     ,  24.169     ,  37.9846    ,\n",
      "        38.714     ,  -2.5528    , -57.585     ,   4.0326    ,\n",
      "        69.008     ,  81.5544    ,   6.506     , -71.6767    ,\n",
      "       -54.426     ,  49.6994    , -84.303     , -57.71222   ,\n",
      "        80.09      ,  86.0264    , -55.428     ,  80.5664    ,\n",
      "        75.733     ]), 'speed': 2.5783409394414765}\n"
     ]
    }
   ],
   "source": [
    "# Creating datasets with all scenarios loaded\n",
    "all_train_val_datasets, all_test_dataset = pedestrian_dataset.create_dataset(\n",
    "    pedestrian_dataset.PedestrianDataType.ALL\n",
    ")\n",
    "\n",
    "# Print the first item from the first train/val dataset part\n",
    "print(all_train_val_datasets[0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'distances': tensor([[ 83.2891, -32.4280,  24.1690,  37.9846,  38.7140,  -2.5528, -57.5850,\n",
      "           4.0326,  69.0080,  81.5544,   6.5060, -71.6767, -54.4260,  49.6994,\n",
      "         -84.3030, -57.7122,  80.0900,  86.0264, -55.4280,  80.5664,  75.7330]],\n",
      "       dtype=torch.float64), 'speed': tensor([2.5783], dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "# Create a PyTorch dataloader with the dataset\n",
    "\n",
    "# TODO: I don't know how to do cross validation, so we combine the first 4 train/val datasets\n",
    "#   to build the train dataset and use the last train/val dataset as the val dataset\n",
    "#   Maybe we just have to do this everytime (and switch it up)? Could be, but not sure\n",
    "all_temp_train_dataset = torch.utils.data.ConcatDataset(all_train_val_datasets[:4])\n",
    "all_temp_val_dataset = all_train_val_datasets[4]\n",
    "\n",
    "all_train_loader = DataLoader(all_temp_train_dataset)\n",
    "all_val_loader = DataLoader(all_temp_train_dataset)\n",
    "\n",
    "# Print the first value given by the train loader\n",
    "for item in all_train_loader:\n",
    "    print(item)\n",
    "    break  # break after printing the first item"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Stuff - TODO\n",
    "\n",
    "Now we need to import our model.\n",
    "\n",
    "\n",
    "\n",
    "Note: Parim already has done some work here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}