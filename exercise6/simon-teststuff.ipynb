{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just my test stuff, will be removed in the end\n",
    "\n",
    "## Readme Data\n",
    "\n",
    "Corridor data are trajectories of pedestrians in a closed corridor of lenght 30m and width 1.8m.\n",
    "The trajectories are measured on a section of length 6m.\n",
    "Experiments are carried out with N=15, 30, 60, 85, 95, 110, 140 and 230 participants.\n",
    "\n",
    "Bottleneck data are trajectories of pedestrian in a bottleneck of lenght 8m and width 1.8m.\n",
    "Experiments are carried out with 150 participants for bottleneck widths w=0.7, 0.95 1.2 and 1.8m.\n",
    "\n",
    "See http://ped.fz-juelich.de/experiments/2009.05.12_Duesseldorf_Messe_Hermes/docu/VersuchsdokumentationHERMES.pdf page 20 and 28 for details. Column names of the file are: ID FRAME X Y Z. ID is the pedestrian ID. FRAME is the frame number (frame rate is 1/16s). X Y and Z pedestrian position in 3D. The data are part of the online database http://ped.fz-juelich.de/database.\n",
    "\n",
    "Column names of the file are: ID FRAME X Y Z.\n",
    "ID is the pedestrian ID.\n",
    "FRAME is the frame number (frame rate is 1/16s).\n",
    "X Y and Z pedestrian position in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Enable these if automatic reloading of modules is wanted\n",
    "\n",
    "# Load extension for automatic reload of modules\n",
    "%load_ext autoreload\n",
    "# Enable autoreload for all modules\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import tensorboard\n",
    "\n",
    "import preprocessing\n",
    "import plotting\n",
    "import pedestrian_dataset\n",
    "import pedestrian_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tensorboard\n",
    "\n",
    "Extension for visualizing the training results.\n",
    "Should only be loaded once, otherwise there is probably an error message.\n",
    "\n",
    "To start, run `tensorboard --logdir=dir --port 6006` in a terminal or run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir .simon/lightning_logs --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "I used Logging to print messages.\n",
    "If more messages are welcome, use the logging level `logging.INFO` or even `logging.DEBUG`.\n",
    "If not, use `logging.WARNING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set Logging Level\n",
    "logger_format = '%(levelname)s - %(funcName)s \\t%(message)s'\n",
    "logger_level = logging.WARNING\n",
    "logging.basicConfig(level=logger_level, format=logger_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set a torch seed\n",
    "torch.manual_seed(1234)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Loading the Dataset\n",
    "\n",
    "The different files of data in the format\n",
    "(`PedID FrameID X Y Z`)\n",
    "are loaded and converted into the following dictionary format:\n",
    "\n",
    "`distances` | `speeds`\n",
    "-|-\n",
    "Input of our neural network. Array of size $2k+1$ containing the median speed of the $k$ nearest neighbors as the first element and the relative $x$- and $y$-positions of the $k$ nearest neighbors in the following pattern afterwards: $x_1$, $y_1$, $x_2$, $y_2$, ... | Truth value for our neural network. The speed that the pedestrian had in that frame.\n",
    "\n",
    "To load a list of files, the method `pedestrian_dataset.create_dataset()` is used.\n",
    "As its first parameter it either takes a list of data files that it should load\n",
    "or a `pedestrian_dataset.PedestrianDataType` value,\n",
    "which can be either `BOTTLENECK`, which loads all bottleneck files,\n",
    "`CORRIDOR`, which loads all corridor files,\n",
    "or `ALL`, which loads all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating datasets with only the smallest corridor scenario with 30 participants\n",
    "c_015_path = \"./Data/Corridor_Data/ug-180-030.txt\"\n",
    "# Note: even when only loading one dataset, it has to be given in a list\n",
    "c_015_train_val_datasets, c_015_test_dataset = pedestrian_dataset.create_dataset([c_015_path])\n",
    "\n",
    "# Print the first item from the first train/val dataset part\n",
    "print(c_015_train_val_datasets[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "# Create a PyTorch dataloader with the dataset\n",
    "\n",
    "# TODO: I don't know how to do cross validation, so we combine the first 4 train/val datasets\n",
    "#   to build the train dataset and use the last train/val dataset as the val dataset\n",
    "#   Maybe we just have to do this everytime (and switch it up)? Could be, but not sure\n",
    "c_015_temp_train_dataset = torch.utils.data.ConcatDataset(c_015_train_val_datasets[:4])\n",
    "c_015_temp_val_dataset = c_015_train_val_datasets[4]\n",
    "\n",
    "c_015_train_loader = DataLoader(c_015_temp_train_dataset, batch_size=batch_size, drop_last=True)\n",
    "c_015_val_loader = DataLoader(c_015_temp_train_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "c_015_test_loader = DataLoader(c_015_test_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "# Print the first value given by the train loader\n",
    "for item in c_015_train_loader:\n",
    "    print(item)\n",
    "    break  # break after printing the first item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Creating datasets with all scenarios loaded\n",
    "all_train_val_datasets, all_test_dataset = pedestrian_dataset.create_dataset(\n",
    "    pedestrian_dataset.PedestrianDataType.ALL\n",
    ")\n",
    "\n",
    "# Print the first item from the first train/val dataset part\n",
    "print(all_train_val_datasets[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# Create a PyTorch dataloader with the dataset\n",
    "\n",
    "# TODO: I don't know how to do cross validation, so we combine the first 4 train/val datasets\n",
    "#   to build the train dataset and use the last train/val dataset as the val dataset\n",
    "#   Maybe we just have to do this everytime (and switch it up)? Could be, but not sure\n",
    "all_temp_train_dataset = torch.utils.data.ConcatDataset(all_train_val_datasets[:4])\n",
    "all_temp_val_dataset = all_train_val_datasets[4]\n",
    "\n",
    "all_train_loader = DataLoader(all_temp_train_dataset, batch_size=batch_size, drop_last=True)\n",
    "all_val_loader = DataLoader(all_temp_train_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "all_test_loader = DataLoader(all_test_dataset, batch_size=batch_size, drop_last=False)\n",
    "\n",
    "# # Currently Disabled because for batch_size=16 this get's large\n",
    "# # Print the first value given by the train loader\n",
    "# for item in all_train_loader:\n",
    "#     print(item)\n",
    "#     break  # break after printing the first item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing and Training the Model\n",
    "\n",
    "Now we need to define our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_name = \"./.simon/checkpoints/2023-07-05--dataAll-ep100-it001.ckpt\"\n",
    "\n",
    "max_epochs = 100\n",
    "k = 10\n",
    "hidden_size = 3\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"CPU will be used.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Choose our dataloaders\n",
    "train_loader = all_train_loader\n",
    "val_loader   = all_val_loader\n",
    "test_loader  = all_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define an early stopping callback\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode='min', patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define our model\n",
    "model = pedestrian_net.PedestrianNet(k=k,\n",
    "                                     hidden_size=hidden_size,\n",
    "                                     learning_rate=learning_rate,\n",
    "                                     optimizer=optimizer\n",
    "                                     )\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    devices='auto',\n",
    "    accelerator='gpu',\n",
    "    callbacks=[early_stop_callback],\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing the Model\n",
    "\n",
    "**After** training and tuning the model, we can test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# trainer.test(model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TODO\n",
    "\n",
    "A list of some things that are still to do.\n",
    "Not a complete list.\n",
    "\n",
    "- [x] Implement Preprocessing\n",
    "- [x] Implement Data Preparation\n",
    "- [x] Implement Model basic structure\n",
    "- [x] Set up basic training for model\n",
    "- [ ] Implement cross validation\n",
    "- [ ] Implement hyper parameter tuning\n",
    "- [ ] ...\n",
    "- [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}